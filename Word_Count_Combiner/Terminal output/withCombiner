bash-4.2$ python wordcount.py -r hadoop --output-dir out --no-output hdfs://andromeda.student.eecs.qmul.ac.uk/user/coa31/input
Using configs in /homes/coa31/.mrjob.conf
Looking for hadoop binary in $PATH...
Found hadoop binary: /bin/hadoop
Using Hadoop version 3.0.0
Looking for Hadoop streaming jar in /home/hadoop/contrib...
Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...
Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar
Creating temp directory /tmp/wordcount.coa31.20191025.130604.398672
Copying local files to hdfs:///user/coa31/tmp/mrjob/wordcount.coa31.20191025.130604.398672/files/...
Running step 1 of 1...
  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.0.0-cdh6.3.0.jar] /tmp/streamjob2057540125950523371.jar tmpDir=null
  Connecting to ResourceManager at andromeda.student.eecs.qmul.ac.uk/138.37.38.58:8032
  Connecting to ResourceManager at andromeda.student.eecs.qmul.ac.uk/138.37.38.58:8032
  Disabling Erasure Coding for path: /user/coa31/.staging/job_1570717177212_0649
  Total input files to process : 1
  number of splits:2
  yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
  Submitting tokens for job: job_1570717177212_0649
  Executing with tokens: []
  resource-types.xml not found
  Unable to find 'resource-types.xml'.
  Submitted application application_1570717177212_0649
  The url to track the job: http://andromeda.student.eecs.qmul.ac.uk:8088/proxy/application_1570717177212_0649/
  Running job: job_1570717177212_0649
  Job job_1570717177212_0649 running in uber mode : false
   map 0% reduce 0%
   map 100% reduce 0%
   map 100% reduce 67%
   map 100% reduce 100%
  Job job_1570717177212_0649 completed successfully
  Output directory: hdfs:///user/coa31/out
Counters: 54
	File Input Format Counters 
		Bytes Read=3026661
	File Output Format Counters 
		Bytes Written=213776
	File System Counters
		FILE: Number of bytes read=184283
		FILE: Number of bytes written=1527898
		FILE: Number of large read operations=0
		FILE: Number of read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3026879
		HDFS: Number of bytes read erasure-coded=0
		HDFS: Number of bytes written=213776
		HDFS: Number of large read operations=0
		HDFS: Number of read operations=21
		HDFS: Number of write operations=6
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=3
		Rack-local map tasks=2
		Total megabyte-milliseconds taken by all map tasks=14995456
		Total megabyte-milliseconds taken by all reduce tasks=14302208
		Total time spent by all map tasks (ms)=14644
		Total time spent by all maps in occupied slots (ms)=14644
		Total time spent by all reduce tasks (ms)=13967
		Total time spent by all reduces in occupied slots (ms)=13967
		Total vcore-milliseconds taken by all map tasks=14644
		Total vcore-milliseconds taken by all reduce tasks=13967
	Map-Reduce Framework
		CPU time spent (ms)=10530
		Combine input records=549644
		Combine output records=25257
		Failed Shuffles=0
		GC time elapsed (ms)=350
		Input split bytes=218
		Map input records=62591
		Map output bytes=5033752
		Map output materialized bytes=216874
		Map output records=549644
		Merged Map outputs=6
		Peak Map Physical memory (bytes)=584777728
		Peak Map Virtual memory (bytes)=2738200576
		Peak Reduce Physical memory (bytes)=343379968
		Peak Reduce Virtual memory (bytes)=2750734336
		Physical memory (bytes) snapshot=2176049152
		Reduce input groups=17032
		Reduce input records=25257
		Reduce output records=17032
		Reduce shuffle bytes=216874
		Shuffled Maps =6
		Spilled Records=50514
		Total committed heap usage (bytes)=2997354496
		Virtual memory (bytes) snapshot=13719912448
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
job output is in hdfs:///user/coa31/out
Removing HDFS temp directory hdfs:///user/coa31/tmp/mrjob/wordcount.coa31.20191025.130604.398672...
Removing temp directory /tmp/wordcount.coa31.20191025.130604.398672...

